openapi: 3.1.1
info:
  title: Stash Auto Vision API
  description: |
    Complete video analysis service providing frame extraction, scene detection,
    face recognition, semantic analysis, and object detection capabilities.

    Architecture: Microservices (6 services) coordinated via Vision API rollup endpoint.

    Services:
    - Frame Server (port 5001) - Frame extraction and caching
    - Scenes Service (port 5002) - Scene boundary detection
    - Faces Service (port 5003) - Face detection, recognition, and quality assessment
    - Semantics Service (port 5004) - CLIP-based scene understanding (stub - Phase 2)
    - Objects Service (port 5005) - YOLO-World object detection (stub - Phase 3)
    - Vision API (port 5010) - Orchestration and rollup endpoint

    For detailed service documentation, see:
    - docs/FRAME_SERVER.md
    - docs/SCENES_SERVICE.md
    - docs/FACES_SERVICE.md
    - docs/SEMANTICS_SERVICE.md
    - docs/OBJECTS_SERVICE.md
    - docs/VISION_API.md
  version: 1.0.0
  contact:
    name: Stash Auto Vision
    url: https://github.com/stashapp/stash-auto-vision
  license:
    name: AGPL-3.0
    url: https://www.gnu.org/licenses/agpl-3.0.html

servers:
  - url: http://localhost:5010
    description: Vision API (Rollup/Orchestrator)
  - url: http://localhost:5001
    description: Frame Server (Internal - not exposed via vision-api)
  - url: http://localhost:5002
    description: Scenes Service
  - url: http://localhost:5003
    description: Faces Service
  - url: http://localhost:5004
    description: Semantics Service (Stub)
  - url: http://localhost:5005
    description: Objects Service (Stub)

tags:
  - name: vision-api
    description: Orchestration and multi-module analysis
  - name: frames
    description: Frame extraction and serving
  - name: scenes
    description: Scene boundary detection
  - name: faces
    description: Face detection, recognition, and quality assessment
  - name: semantics
    description: Semantic scene understanding (stub)
  - name: objects
    description: Object detection and classification (stub)
  - name: health
    description: Service health checks

paths:
  # ==================== VISION API (Rollup) ====================

  /vision/analyze:
    post:
      tags: [vision-api]
      summary: Analyze video (multi-module)
      description: Submit a video for comprehensive analysis across multiple modules
      operationId: analyzeVideo
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/AnalyzeRequest'
      responses:
        '202':
          description: Analysis job accepted
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AnalyzeJobResponse'
        '400':
          $ref: '#/components/responses/BadRequest'
        '404':
          $ref: '#/components/responses/NotFound'

  /vision/jobs/{job_id}/status:
    get:
      tags: [vision-api]
      summary: Get analysis job status
      operationId: getAnalysisStatus
      parameters:
        - $ref: '#/components/parameters/JobId'
      responses:
        '200':
          description: Job status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AnalyzeJobStatus'
        '404':
          $ref: '#/components/responses/NotFound'

  /vision/jobs/{job_id}/results:
    get:
      tags: [vision-api]
      summary: Get analysis results
      operationId: getAnalysisResults
      parameters:
        - $ref: '#/components/parameters/JobId'
      responses:
        '200':
          description: Analysis results
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/AnalyzeResults'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          description: Job not completed yet

  /vision/jobs:
    get:
      tags: [vision-api]
      summary: List all jobs
      description: |
        List all jobs across all services (vision-api, faces-service, scenes-service)
        with optional filtering and pagination.

        Jobs are deduplicated by cache_key, preferring vision-level jobs over service-level jobs.
      operationId: listJobs
      parameters:
        - name: status
          in: query
          schema:
            type: string
            enum: [queued, processing, completed, failed]
          description: Filter by job status
        - name: service
          in: query
          schema:
            type: string
            enum: [vision, faces, scenes]
          description: Filter by service (default is all services)
        - name: source_id
          in: query
          schema:
            type: string
          description: Filter by source_id
        - name: source
          in: query
          schema:
            type: string
          description: Filter by source video path
        - name: start_date
          in: query
          schema:
            type: string
            format: date-time
          description: Filter by start date (ISO format, e.g., 2025-01-01T00:00:00Z)
        - name: end_date
          in: query
          schema:
            type: string
            format: date-time
          description: Filter by end date (ISO format)
        - name: include_results
          in: query
          schema:
            type: boolean
            default: false
          description: Include full job results in response
        - name: limit
          in: query
          schema:
            type: integer
            minimum: 1
            maximum: 500
            default: 50
          description: Maximum number of jobs to return
        - name: offset
          in: query
          schema:
            type: integer
            minimum: 0
            default: 0
          description: Number of jobs to skip (for pagination)
      responses:
        '200':
          description: List of jobs
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListJobsResponse'
        '400':
          $ref: '#/components/responses/BadRequest'

  /health:
    get:
      tags: [health, vision-api]
      summary: Vision API health check
      operationId: visionApiHealth
      responses:
        '200':
          description: Service health with all downstream services
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VisionApiHealth'

  # ==================== FRAME SERVER ====================

  /extract:
    post:
      tags: [frames]
      summary: Extract frames from video
      operationId: extractFrames
      servers:
        - url: http://localhost:5001
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ExtractRequest'
      responses:
        '202':
          description: Extraction job accepted
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExtractJobResponse'
        '400':
          $ref: '#/components/responses/BadRequest'
        '404':
          $ref: '#/components/responses/NotFound'

  /jobs/{job_id}/status:
    get:
      tags: [frames]
      summary: Get extraction job status
      operationId: getExtractionStatus
      servers:
        - url: http://localhost:5001
      parameters:
        - $ref: '#/components/parameters/JobId'
      responses:
        '200':
          description: Job status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExtractJobStatus'
        '404':
          $ref: '#/components/responses/NotFound'

  /jobs/{job_id}/results:
    get:
      tags: [frames]
      summary: Get extraction results
      operationId: getExtractionResults
      servers:
        - url: http://localhost:5001
      parameters:
        - $ref: '#/components/parameters/JobId'
      responses:
        '200':
          description: Extraction results
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExtractResults'
        '404':
          $ref: '#/components/responses/NotFound'

  /frames/{job_id}/{frame_index}:
    get:
      tags: [frames]
      summary: Get specific frame
      operationId: getFrame
      servers:
        - url: http://localhost:5001
      parameters:
        - name: job_id
          in: path
          required: true
          schema:
            type: string
        - name: frame_index
          in: path
          required: true
          schema:
            type: integer
            minimum: 0
        - name: wait
          in: query
          schema:
            type: boolean
            default: true
          description: Wait up to 30s for frame to be extracted
      responses:
        '200':
          description: Frame image
          headers:
            X-Frame-Index:
              schema:
                type: integer
          content:
            image/jpeg:
              schema:
                type: string
                format: binary
            image/png:
              schema:
                type: string
                format: binary
        '202':
          description: Extraction in progress (if wait=false)
        '400':
          $ref: '#/components/responses/BadRequest'
        '404':
          $ref: '#/components/responses/NotFound'
        '408':
          description: Timeout waiting for frame

  /extract-frame:
    get:
      tags: [frames]
      summary: Extract single frame
      description: Extract a single frame without job tracking (for thumbnails/previews)
      operationId: extractSingleFrame
      servers:
        - url: http://localhost:5001
      parameters:
        - name: video_path
          in: query
          required: true
          schema:
            type: string
        - name: timestamp
          in: query
          required: true
          schema:
            type: number
        - name: output_format
          in: query
          schema:
            type: string
            enum: [jpeg, png]
            default: jpeg
        - name: quality
          in: query
          schema:
            type: integer
            minimum: 1
            maximum: 100
            default: 95
        - name: enhance
          in: query
          schema:
            type: integer
            enum: [0, 1]
            default: 0
          description: Enable face enhancement (0=disabled, 1=enabled)
        - name: model
          in: query
          schema:
            type: string
            enum: [gfpgan, codeformer]
            default: gfpgan
          description: Enhancement model to use
        - name: fidelity_weight
          in: query
          schema:
            type: number
            minimum: 0
            maximum: 1
            default: 0.7
          description: Enhancement fidelity (0.0=preserve original, 1.0=max enhancement)
      responses:
        '200':
          description: Frame image
          headers:
            X-Timestamp:
              schema:
                type: number
            X-Frame-Number:
              schema:
                type: integer
          content:
            image/jpeg:
              schema:
                type: string
                format: binary
            image/png:
              schema:
                type: string
                format: binary
        '400':
          $ref: '#/components/responses/BadRequest'
        '404':
          $ref: '#/components/responses/NotFound'

  # ==================== SCENES SERVICE ====================

  /detect:
    post:
      tags: [scenes]
      summary: Detect scene boundaries
      operationId: detectScenes
      servers:
        - url: http://localhost:5002
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/DetectScenesRequest'
      responses:
        '202':
          description: Detection job accepted
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DetectJobResponse'
        '400':
          $ref: '#/components/responses/BadRequest'
        '404':
          $ref: '#/components/responses/NotFound'

  /jobs/{job_id}/status:
    get:
      tags: [scenes]
      summary: Get scene detection job status
      operationId: getSceneDetectionStatus
      servers:
        - url: http://localhost:5002
      parameters:
        - $ref: '#/components/parameters/JobId'
      responses:
        '200':
          description: Job status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DetectJobStatus'
        '404':
          $ref: '#/components/responses/NotFound'

  /jobs/{job_id}/results:
    get:
      tags: [scenes]
      summary: Get scene detection results
      operationId: getSceneDetectionResults
      servers:
        - url: http://localhost:5002
      parameters:
        - $ref: '#/components/parameters/JobId'
      responses:
        '200':
          description: Detection results
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DetectJobResults'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          description: Job not completed yet

  # ==================== FACES SERVICE ====================

  /analyze:
    post:
      tags: [faces]
      summary: Analyze faces in video
      operationId: analyzeFaces
      servers:
        - url: http://localhost:5003
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/AnalyzeFacesRequest'
      responses:
        '202':
          description: Analysis job accepted
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/FacesJobResponse'
        '400':
          $ref: '#/components/responses/BadRequest'
        '404':
          $ref: '#/components/responses/NotFound'

  /jobs/{job_id}/status:
    get:
      tags: [faces]
      summary: Get face analysis job status
      operationId: getFaceAnalysisStatus
      servers:
        - url: http://localhost:5003
      parameters:
        - $ref: '#/components/parameters/JobId'
      responses:
        '200':
          description: Job status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/FacesJobStatus'
        '404':
          $ref: '#/components/responses/NotFound'

  /jobs/{job_id}/results:
    get:
      tags: [faces]
      summary: Get face analysis results
      operationId: getFaceAnalysisResults
      servers:
        - url: http://localhost:5003
      parameters:
        - $ref: '#/components/parameters/JobId'
      responses:
        '200':
          description: Analysis results
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/FacesJobResults'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          description: Job not completed yet

components:
  parameters:
    JobId:
      name: job_id
      in: path
      required: true
      schema:
        type: string
        format: uuid
      description: Job identifier

  responses:
    BadRequest:
      description: Invalid request
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
    NotFound:
      description: Resource not found
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'

  schemas:
    Error:
      type: object
      properties:
        detail:
          type: string

    # Vision API Schemas
    AnalyzeRequest:
      type: object
      required:
        - source
        - source_id
      properties:
        source:
          type: string
          description: Path, URL, or image source to analyze
          example: /media/videos/scene.mp4
        source_type:
          type: string
          enum: [video, image, url]
          description: Source type (auto-detected if omitted)
        source_id:
          type: string
          description: Source identifier
        job_id:
          type: string
          description: Job ID for tracking
        processing_mode:
          type: string
          enum: [sequential, parallel]
          default: sequential
          description: Processing mode (sequential recommended to avoid GPU memory contention)
        modules:
          type: object
          description: Module configuration with explicit parameter schemas
          properties:
            scenes:
              type: object
              properties:
                enabled:
                  type: boolean
                  default: true
                  description: Enable scene detection
                parameters:
                  allOf:
                    - $ref: '#/components/schemas/ScenesParameters'
            faces:
              type: object
              properties:
                enabled:
                  type: boolean
                  default: true
                  description: Enable face recognition
                parameters:
                  allOf:
                    - $ref: '#/components/schemas/FacesParameters'
            semantics:
              type: object
              properties:
                enabled:
                  type: boolean
                  default: false
                  description: Enable CLIP semantic analysis (Phase 2 - not implemented)
                parameters:
                  type: object
                  description: Semantic analysis parameters (Phase 2)
            objects:
              type: object
              properties:
                enabled:
                  type: boolean
                  default: false
                  description: Enable YOLO object detection (Phase 3 - not implemented)
                parameters:
                  type: object
                  description: Object detection parameters (Phase 3)

    AnalyzeJobResponse:
      type: object
      properties:
        job_id:
          type: string
          format: uuid
        status:
          type: string
          enum: [queued, processing, completed, failed]
        created_at:
          type: string
          format: date-time
        services_enabled:
          type: object
          properties:
            scenes:
              type: boolean
            faces:
              type: boolean
            semantics:
              type: boolean
            objects:
              type: boolean
        processing_mode:
          type: string
          enum: [sequential, parallel]

    AnalyzeJobStatus:
      type: object
      properties:
        job_id:
          type: string
        status:
          type: string
          enum: [queued, processing, completed, failed]
        progress:
          type: number
          minimum: 0
          maximum: 1
        processing_mode:
          type: string
        services:
          type: array
          items:
            type: object
        created_at:
          type: string
          format: date-time
        started_at:
          type: string
          format: date-time
          nullable: true
        completed_at:
          type: string
          format: date-time
          nullable: true
        error:
          type: string
          nullable: true

    AnalyzeResults:
      type: object
      properties:
        job_id:
          type: string
        source_id:
          type: string
        status:
          type: string
        scenes:
          type: object
          nullable: true
        faces:
          type: object
          nullable: true
        semantics:
          type: object
          nullable: true
        objects:
          type: object
          nullable: true
        metadata:
          type: object

    VisionApiHealth:
      type: object
      properties:
        status:
          type: string
        service:
          type: string
        version:
          type: string
        services:
          type: object
          additionalProperties:
            type: object

    ListJobsResponse:
      type: object
      properties:
        jobs:
          type: array
          items:
            $ref: '#/components/schemas/JobSummary'
        total:
          type: integer
          description: Total number of jobs matching filters (before pagination)
        limit:
          type: integer
        offset:
          type: integer

    JobSummary:
      type: object
      properties:
        job_id:
          type: string
        service:
          type: string
          enum: [vision, faces, scenes]
          description: Service that owns the job
        status:
          type: string
          enum: [queued, processing, completed, failed]
        progress:
          type: number
          minimum: 0.0
          maximum: 1.0
        source:
          type: string
          nullable: true
        source_id:
          type: string
          nullable: true
        created_at:
          type: string
          format: date-time
          nullable: true
        started_at:
          type: string
          format: date-time
          nullable: true
        completed_at:
          type: string
          format: date-time
          nullable: true
        result_summary:
          type: object
          nullable: true
        results:
          type: object
          nullable: true
          description: Full job results (only when include_results=true)

    # Frame Server Schemas
    ExtractRequest:
      type: object
      required:
        - video_path
      properties:
        video_path:
          type: string
        job_id:
          type: string
          description: Parent job ID for tracking
        extraction_method:
          type: string
          enum: [opencv_cpu, opencv_cuda, ffmpeg, sprites]
          default: opencv_cpu
        sampling_strategy:
          type: object
          properties:
            mode:
              type: string
              enum: [interval, keyframe, uniform]
              default: interval
            interval_seconds:
              type: number
            num_frames:
              type: integer
        scene_boundaries:
          type: array
          description: Scene boundaries from scenes-service for targeted extraction
          items:
            type: object
            properties:
              start_timestamp:
                type: number
              end_timestamp:
                type: number
        output_format:
          type: string
          enum: [jpeg, png]
          default: jpeg
        quality:
          type: integer
          minimum: 1
          maximum: 100
          default: 95
        cache_duration:
          type: integer
          default: 7200
          description: Cache TTL in seconds
        use_sprites:
          type: boolean
          default: false
        sprite_vtt_url:
          type: string
        sprite_image_url:
          type: string
        enhancement:
          $ref: '#/components/schemas/FrameEnhancementOptions'

    FrameEnhancementModel:
      type: string
      enum: [gfpgan, codeformer]
      description: Face enhancement model

    FrameEnhancementOptions:
      type: object
      properties:
        enabled:
          type: boolean
          default: false
          description: Enable face enhancement
        model:
          $ref: '#/components/schemas/FrameEnhancementModel'
          default: gfpgan
        fidelity_weight:
          type: number
          default: 0.7
          minimum: 0
          maximum: 1
          description: Balance between quality and fidelity (0=preserve original, 1=max enhancement)
        upscale:
          type: integer
          default: 2
          minimum: 1
          maximum: 4
          description: Upscaling factor
        only_center_face:
          type: boolean
          default: false
          description: Only enhance the most centered face
        paste_back:
          type: boolean
          default: true
          description: Paste enhanced faces back to original image

    ExtractJobResponse:
      type: object
      properties:
        job_id:
          type: string
        status:
          type: string
        created_at:
          type: string
          format: date-time
        cache_key:
          type: string
        estimated_frames:
          type: integer

    ExtractJobStatus:
      type: object
      properties:
        job_id:
          type: string
        status:
          type: string
        progress:
          type: number
          minimum: 0.0
          maximum: 1.0
        stage:
          type: string
          nullable: true
        message:
          type: string
          nullable: true
        frames_extracted:
          type: integer
          nullable: true
        frames_total:
          type: integer
          nullable: true
        created_at:
          type: string
          format: date-time
        started_at:
          type: string
          format: date-time
          nullable: true
        completed_at:
          type: string
          format: date-time
          nullable: true
        error:
          type: string
          nullable: true

    ExtractResults:
      type: object
      properties:
        job_id:
          type: string
        status:
          type: string
        cache_key:
          type: string
        frames:
          type: array
          items:
            $ref: '#/components/schemas/FrameMetadata'
        metadata:
          $ref: '#/components/schemas/VideoMetadata'

    FrameMetadata:
      type: object
      properties:
        index:
          type: integer
        timestamp:
          type: number
        url:
          type: string
        width:
          type: integer
        height:
          type: integer

    VideoMetadata:
      type: object
      properties:
        video_path:
          type: string
        extraction_method:
          type: string
        total_frames:
          type: integer
        video_duration_seconds:
          type: number
        video_fps:
          type: number
        processing_time_seconds:
          type: number
        enhancement_enabled:
          type: boolean
          default: false
          description: Whether face enhancement was enabled for this extraction
        enhancement_model:
          type: string
          nullable: true
          description: Enhancement model used (gfpgan or codeformer), null if enhancement disabled
        faces_enhanced:
          type: integer
          nullable: true
          description: Total number of faces enhanced across all frames, null if enhancement disabled

    # Scenes Service Schemas
    DetectScenesRequest:
      type: object
      required:
        - video_path
      properties:
        video_path:
          type: string
        source_id:
          type: string
        job_id:
          type: string
          description: Parent job ID for tracking
        detection_method:
          type: string
          enum: [content, threshold, adaptive]
          default: content
        scene_threshold:
          type: number
          default: 27.0
          minimum: 0.0
          maximum: 100.0
          description: Detection threshold
        min_scene_length:
          type: number
          default: 0.6
          minimum: 0.1
          description: Minimum scene length in seconds
        cache_duration:
          type: integer
          default: 3600
          minimum: 0
          description: Cache TTL in seconds

    ScenesParameters:
      type: object
      properties:
        detection_method:
          type: string
          enum: [content, threshold, adaptive]
          default: content
        scene_threshold:
          type: number
          default: 27.0
          minimum: 0.0
          maximum: 100.0
          description: Content detection threshold
        min_scene_length:
          type: number
          default: 0.6
          minimum: 0.1
          description: Minimum scene duration in seconds

    DetectJobResponse:
      type: object
      properties:
        job_id:
          type: string
        status:
          type: string
        created_at:
          type: string
          format: date-time
        cache_key:
          type: string
        estimated_scenes:
          type: integer
          nullable: true

    DetectJobStatus:
      type: object
      properties:
        job_id:
          type: string
        status:
          type: string
        progress:
          type: number
          minimum: 0.0
          maximum: 1.0
        stage:
          type: string
          nullable: true
        message:
          type: string
          nullable: true
        scenes_detected:
          type: integer
          nullable: true
        created_at:
          type: string
          format: date-time
        started_at:
          type: string
          format: date-time
          nullable: true
        completed_at:
          type: string
          format: date-time
          nullable: true
        error:
          type: string
          nullable: true

    DetectJobResults:
      type: object
      properties:
        job_id:
          type: string
        status:
          type: string
        cache_key:
          type: string
        scenes:
          type: array
          items:
            $ref: '#/components/schemas/SceneBoundary'
        metadata:
          $ref: '#/components/schemas/ScenesVideoMetadata'

    SceneBoundary:
      type: object
      properties:
        scene_number:
          type: integer
        start_frame:
          type: integer
        end_frame:
          type: integer
        start_timestamp:
          type: number
        end_timestamp:
          type: number
        duration:
          type: number

    ScenesVideoMetadata:
      type: object
      properties:
        video_path:
          type: string
        detection_method:
          type: string
        total_frames:
          type: integer
        video_duration_seconds:
          type: number
        video_fps:
          type: number
        processing_time_seconds:
          type: number

    # Faces Service Schemas
    AnalyzeFacesRequest:
      type: object
      required:
        - source
        - source_id
      properties:
        source:
          type: string
          description: Path, URL, or image source to analyze
        source_type:
          type: string
          enum: [video, image, url]
          description: Source type (auto-detected if omitted)
        original_source:
          type: string
          description: Original source path/URL before download (for metadata)
        source_id:
          type: string
          description: Source ID for reference
        job_id:
          type: string
          description: Parent job ID for tracking
        parameters:
          $ref: '#/components/schemas/FacesParameters'

    FacesParameters:
      type: object
      properties:
        face_min_confidence:
          type: number
          default: 0.9
          minimum: 0
          maximum: 1
          description: Minimum detection confidence threshold (default configurable via FACES_MIN_CONFIDENCE env var)
        face_min_quality:
          type: number
          default: 0.0
          minimum: 0
          maximum: 1
          description: Minimum quality threshold (filter faces below this, default configurable via FACES_MIN_QUALITY env var)
        max_faces:
          type: integer
          default: 50
          minimum: 1
          maximum: 1000
        sampling_interval:
          type: number
          default: 2.0
          minimum: 0.1
          maximum: 10.0
          description: Seconds between sampled frames
        use_sprites:
          type: boolean
          default: false
          description: Use sprite sheets if available
        sprite_vtt_url:
          type: string
          description: URL to WebVTT sprite manifest
        sprite_image_url:
          type: string
          description: URL to sprite sheet image
        enable_deduplication:
          type: boolean
          default: true
          description: Cluster faces by embedding similarity
        embedding_similarity_threshold:
          type: number
          default: 0.6
          minimum: 0
          maximum: 1
          description: Cosine similarity threshold for clustering
        detect_demographics:
          type: boolean
          default: true
          description: Detect age, gender, emotion
        scene_boundaries:
          type: array
          description: Scene boundaries for targeted face detection
          items:
            type: object
            properties:
              start_timestamp:
                type: number
              end_timestamp:
                type: number
        cache_duration:
          type: integer
          default: 3600
          minimum: 0
          description: Cache TTL in seconds
        enhancement:
          $ref: '#/components/schemas/EnhancementParameters'

    EnhancementParameters:
      type: object
      properties:
        enabled:
          type: boolean
          default: false
          description: Enable face enhancement via CodeFormer/GFPGAN
        quality_trigger:
          type: number
          default: 0.5
          minimum: 0
          maximum: 1
          description: Trigger enhancement if quality score below this threshold (default configurable via FACES_ENHANCEMENT_QUALITY_TRIGGER env var)
        model:
          type: string
          enum: [gfpgan, codeformer]
          default: codeformer
          description: Enhancement model to use
        fidelity_weight:
          type: number
          default: 0.5
          minimum: 0
          maximum: 1
          description: Fidelity vs quality tradeoff (0=quality, 1=fidelity)

    FacesJobResponse:
      type: object
      properties:
        job_id:
          type: string
        status:
          type: string
        created_at:
          type: string
          format: date-time

    FacesJobStatus:
      type: object
      properties:
        job_id:
          type: string
        status:
          type: string
        progress:
          type: number
          minimum: 0.0
          maximum: 1.0
        stage:
          type: string
          nullable: true
        message:
          type: string
          nullable: true
        created_at:
          type: string
          format: date-time
        started_at:
          type: string
          format: date-time
          nullable: true
        completed_at:
          type: string
          format: date-time
          nullable: true
        result_summary:
          type: object
          nullable: true
          properties:
            unique_faces:
              type: integer
            total_detections:
              type: integer
            frames_processed:
              type: integer
            processing_time_seconds:
              type: number
        error:
          type: string
          nullable: true

    FacesJobResults:
      type: object
      properties:
        job_id:
          type: string
        source_id:
          type: string
        status:
          type: string
        faces:
          type: array
          items:
            $ref: '#/components/schemas/Face'
        metadata:
          $ref: '#/components/schemas/FacesVideoMetadata'

    Face:
      type: object
      properties:
        face_id:
          type: string
        embedding:
          type: array
          items:
            type: number
          description: 512-D ArcFace embedding
        demographics:
          $ref: '#/components/schemas/Demographics'
        detections:
          type: array
          items:
            $ref: '#/components/schemas/Detection'
        representative_detection:
          $ref: '#/components/schemas/Detection'

    Detection:
      type: object
      properties:
        frame_index:
          type: integer
        timestamp:
          type: number
        bbox:
          $ref: '#/components/schemas/BoundingBox'
        confidence:
          type: number
        quality:
          $ref: '#/components/schemas/Quality'
        pose:
          type: string
          description: "front, left, right, front-rotate-left, front-rotate-right"
        landmarks:
          $ref: '#/components/schemas/Landmarks'
        enhanced:
          type: boolean
          default: false
          description: Indicates if this face was enhanced via CodeFormer/GFPGAN
        occlusion:
          $ref: '#/components/schemas/Occlusion'

    Quality:
      type: object
      properties:
        composite:
          type: number
          minimum: 0.0
          maximum: 1.0
          description: Overall quality score (0.0-1.0)
        components:
          $ref: '#/components/schemas/QualityComponents'

    QualityComponents:
      type: object
      properties:
        size:
          type: number
          minimum: 0.0
          maximum: 1.0
          description: Face size score (0.0-1.0)
        pose:
          type: number
          minimum: 0.0
          maximum: 1.0
          description: Face pose score (0.0-1.0)
        occlusion:
          type: number
          minimum: 0.0
          maximum: 1.0
          description: Occlusion score (0.0-1.0)
        sharpness:
          type: number
          minimum: 0.0
          maximum: 1.0
          description: Sharpness/IQA score (0.0-1.0) via TOPIQ/CLIP-IQA

    Occlusion:
      type: object
      properties:
        occluded:
          type: boolean
          description: Whether face is occluded (glasses, mask, hand, etc.)
        probability:
          type: number
          minimum: 0.0
          maximum: 1.0
          description: Occlusion probability (0.0-1.0)

    BoundingBox:
      type: object
      properties:
        x_min:
          type: integer
        y_min:
          type: integer
        x_max:
          type: integer
        y_max:
          type: integer

    Landmarks:
      type: object
      properties:
        left_eye:
          type: array
          items:
            type: integer
          minItems: 2
          maxItems: 2
        right_eye:
          type: array
          items:
            type: integer
          minItems: 2
          maxItems: 2
        nose:
          type: array
          items:
            type: integer
          minItems: 2
          maxItems: 2
        mouth_left:
          type: array
          items:
            type: integer
          minItems: 2
          maxItems: 2
        mouth_right:
          type: array
          items:
            type: integer
          minItems: 2
          maxItems: 2

    Demographics:
      type: object
      nullable: true
      properties:
        age:
          type: integer
        gender:
          type: string
          enum: [M, F]
        emotion:
          type: string
          enum: [neutral, happy, sad, angry, surprise, disgust, fear]

    FacesVideoMetadata:
      type: object
      properties:
        source:
          type: string
          description: Original source path or URL
        source_type:
          type: string
          enum: [video, image, url]
          description: Detected source type
        total_frames:
          type: integer
        frames_processed:
          type: integer
        unique_faces:
          type: integer
        total_detections:
          type: integer
        processing_time_seconds:
          type: number
        method:
          type: string
        model:
          type: string
        frame_enhancement:
          allOf:
            - $ref: '#/components/schemas/EnhancementParameters'
          nullable: true
          description: Frame enhancement settings used (null if enhancement was disabled)
